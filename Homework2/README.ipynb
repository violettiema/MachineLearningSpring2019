{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================================================================================================\n",
    "\n",
    "# MACHINE LEARNING\n",
    "\n",
    "## HOMEWORK 2\n",
    "\n",
    "### VIOLET TIEMA\n",
    "\n",
    "### February 2019\n",
    "\n",
    "\n",
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a ipynb file that runs on jupyter notebook. I have included the solutions to Problem 4 within the file at various areas where they are required and also have them here. \n",
    "\n",
    "\n",
    "\n",
    "# QUESTION 2 : \n",
    "\n",
    "## Observations the Scaled Boston Housing Data\n",
    "\n",
    "Based on the above results, we can see that the best lambda value would be $\\lambda$ = 10 since it produced the highest accuracy of 0.1554375448284402, and the least RMSE value. We also notice that as we increase the lambda value past the optmimum, the accuracy decreases.\n",
    "\n",
    "In practice, ridge regression is usually the first choice between these two models.\n",
    "\n",
    "However, if you have a large amount of features and expect only a few of them to be important, Lasso might be a better choice. Similarly, if you would like to have a model that is easy to interpret, Lasso will provide a model that is easier to understand, as it will select only a subset of the input features.\n",
    "\n",
    "\n",
    "# QUESTION 4 :\n",
    "\n",
    "## Observations the Original Boston Housing Data\n",
    "For the lasso, we managed a root mean squared error value of about 5.5, while the Ridge resulted in RMSE of 4.9. \n",
    "Clearly the Ridge regression was the better choice for thsi particular data. If we compare it to the unscaled data, we see that the lasso has a value of 4.76 which is a clear drop from the previous 5.5. On the other hand the Ridge registered a RMSE of 4.9 which was about the same as the scaled data. I believe that this is a clear indication that the ridge model is way better as compared to the lasso. \n",
    "\n",
    "I have wrutten down a few more conclusions that I drew as i was working on the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CONCLUSIONS\n",
    "\n",
    "\n",
    "### Key Difference\n",
    "**Ridge**: It includes all (or none) of the features in the model. Thus, the major advantage of ridge regression is coefficient shrinkage and reducing model complexity.\n",
    "\n",
    "**Lasso**: Along with shrinking coefficients, lasso performs feature selection as well. (Remember the ‘selection‘ in the lasso full-form?) As we observed earlier, some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model.\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "### Typical Use Cases\n",
    "**Ridge**: It is majorly used to prevent overfitting. Since it includes all the features, it is not very useful in case of exorbitantly high #features, say in millions, as it will pose computational challenges.\n",
    "\n",
    "**Lasso**: Since it provides sparse solutions, it is generally the model of choice (or some variant of this concept) for modelling cases where the #features are in millions or more. In such a case, getting a sparse solution is of great computational advantage as the features with zero coefficients can simply be ignored.\n",
    "Its not hard to see why the stepwise selection techniques become practically very cumbersome to implement in high dimensionality cases. Thus, lasso provides a significant advantage.\n",
    "\n",
    "### Presence of Highly Correlated Features\n",
    "**Ridge**: It generally works well even in presence of highly correlated features as it will include all of them in the model but the coefficients will be distributed among them depending on the correlation.\n",
    "\n",
    "**Lasso**: It arbitrarily selects any one feature among the highly correlated ones and reduced the coefficients of the rest to zero. Also, the chosen variable changes randomly with change in model parameters. This generally doesn’t work that well as compared to ridge regression.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
